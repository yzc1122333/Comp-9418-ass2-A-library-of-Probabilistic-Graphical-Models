{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "w_o_residual_structure_of_BAA_final_MRSAnet_34__4.728__Demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "509b3f981dc64e1b823220f3e086468d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d658b2a132974dbc96808654265c539b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d1d38755703a4db4b2ced4f99fe5e5ed",
              "IPY_MODEL_d188296bbc65413dbeab6a3d14a57614"
            ]
          }
        },
        "d658b2a132974dbc96808654265c539b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d1d38755703a4db4b2ced4f99fe5e5ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6c272e4d269f4af7b2db4af48ac3585f",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 87306240,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 87306240,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_22e8ed338038412fba8f81f46ee21d22"
          }
        },
        "d188296bbc65413dbeab6a3d14a57614": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c2cb45bd8446493aa856387b0b04cae6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 83.3M/83.3M [00:04&lt;00:00, 18.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1cb7b94a87f7451196bd97083d68edc0"
          }
        },
        "6c272e4d269f4af7b2db4af48ac3585f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "22e8ed338038412fba8f81f46ee21d22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c2cb45bd8446493aa856387b0b04cae6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1cb7b94a87f7451196bd97083d68edc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yzc1122333/Comp-9418-ass2-A-library-of-Probabilistic-Graphical-Models/blob/master/w_o_residual_structure_of_BAA_final_MRSAnet_34__4_728__Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vg2N16KFlrHZ",
        "outputId": "f1f41710-1778-4930-ded7-dcca54d8c74d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_8OjcJMY61e",
        "outputId": "efb14990-c13d-4ba7-e3c7-f75dca01ebaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --version 20201010 --apt-packages libomp5 libopenblas-dev"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  5116  100  5116    0     0  29744      0 --:--:-- --:--:-- --:--:-- 29744\n",
            "Updating... This may take around 2 minutes.\n",
            "Updating TPU runtime to pytorch-dev20201010 ...\n",
            "Collecting cloud-tpu-client\n",
            "  Downloading https://files.pythonhosted.org/packages/56/9f/7b1958c2886db06feb5de5b2c191096f9e619914b6c31fdf93999fdbbd8b/cloud_tpu_client-0.10-py3-none-any.whl\n",
            "Collecting google-api-python-client==1.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/b4/a955f393b838bc47cbb6ae4643b9d0f90333d3b4db4dc1e819f36aad18cc/google_api_python_client-1.8.0-py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.6/dist-packages (from cloud-tpu-client) (4.1.3)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (0.17.4)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.15.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (3.0.1)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.16.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (0.0.4)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.17.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.2.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (4.6)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.4.8)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2018.9)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.12.4)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.52.0)\n",
            "Requirement already satisfied: setuptools>=34.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (50.3.2)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.23.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client) (4.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2020.6.20)\n",
            "Uninstalling torch-1.7.0+cu101:\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.24.3)\n",
            "Installing collected packages: google-api-python-client, cloud-tpu-client\n",
            "  Found existing installation: google-api-python-client 1.7.12\n",
            "    Uninstalling google-api-python-client-1.7.12:\n",
            "      Successfully uninstalled google-api-python-client-1.7.12\n",
            "Successfully installed cloud-tpu-client-0.10 google-api-python-client-1.8.0\n",
            "Done updating TPU runtime\n",
            "  Successfully uninstalled torch-1.7.0+cu101\n",
            "Uninstalling torchvision-0.8.1+cu101:\n",
            "  Successfully uninstalled torchvision-0.8.1+cu101\n",
            "Copying gs://tpu-pytorch/wheels/torch-nightly+20201010-cp36-cp36m-linux_x86_64.whl...\n",
            "- [1 files][114.8 MiB/114.8 MiB]                                                \n",
            "Operation completed over 1 objects/114.8 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torch_xla-nightly+20201010-cp36-cp36m-linux_x86_64.whl...\n",
            "- [1 files][127.5 MiB/127.5 MiB]                                                \n",
            "Operation completed over 1 objects/127.5 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torchvision-nightly+20201010-cp36-cp36m-linux_x86_64.whl...\n",
            "/ [1 files][  3.0 MiB/  3.0 MiB]                                                \n",
            "Operation completed over 1 objects/3.0 MiB.                                      \n",
            "Processing ./torch-nightly+20201010-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==nightly+20201010) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==nightly+20201010) (1.18.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch==nightly+20201010) (0.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==nightly+20201010) (3.7.4.3)\n",
            "\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-1.8.0a0\n",
            "Processing ./torch_xla-nightly+20201010-cp36-cp36m-linux_x86_64.whl\n",
            "Installing collected packages: torch-xla\n",
            "Successfully installed torch-xla-1.6+f631495\n",
            "Processing ./torchvision-nightly+20201010-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20201010) (1.18.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20201010) (1.8.0a0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20201010) (7.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==nightly+20201010) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==nightly+20201010) (0.16.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==nightly+20201010) (0.7)\n",
            "Installing collected packages: torchvision\n",
            "Successfully installed torchvision-0.8.0a0+d537965\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libopenblas-dev is already the newest version (0.2.20+ds-4).\n",
            "The following NEW packages will be installed:\n",
            "  libomp5\n",
            "0 upgraded, 1 newly installed, 0 to remove and 11 not upgraded.\n",
            "Need to get 234 kB of archives.\n",
            "After this operation, 774 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\n",
            "Fetched 234 kB in 1s (333 kB/s)\n",
            "Selecting previously unselected package libomp5:amd64.\n",
            "(Reading database ... 144628 files and directories currently installed.)\n",
            "Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\n",
            "Unpacking libomp5:amd64 (5.0.1-1) ...\n",
            "Setting up libomp5:amd64 (5.0.1-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJATr3IgUqWY",
        "outputId": "105e2ebe-d85b-4172-8b74-60618d31b31b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install pretrainedmodels"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pretrainedmodels\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/0e/be6a0e58447ac16c938799d49bfb5fb7a80ac35e137547fc6cee2c08c4cf/pretrainedmodels-0.7.4.tar.gz (58kB)\n",
            "\r\u001b[K     |█████▋                          | 10kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 30kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 51kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 2.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (1.8.0a0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (0.8.0a0+d537965)\n",
            "Collecting munch\n",
            "  Downloading https://files.pythonhosted.org/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (4.41.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->pretrainedmodels) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->pretrainedmodels) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->pretrainedmodels) (1.18.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch->pretrainedmodels) (0.7)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->pretrainedmodels) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from munch->pretrainedmodels) (1.15.0)\n",
            "Building wheels for collected packages: pretrainedmodels\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-cp36-none-any.whl size=60964 sha256=e64f1efb957b31a188e84f7481a9f8c0bd06f3a01c1de7b83abe8a24759f7782\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/df/63/62583c096289713f22db605aa2334de5b591d59861a02c2ecd\n",
            "Successfully built pretrainedmodels\n",
            "Installing collected packages: munch, pretrainedmodels\n",
            "Successfully installed munch-2.5.0 pretrainedmodels-0.7.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkxZXP1bvFKV",
        "outputId": "ed13ff9d-c48c-4bac-c574-fa48a68a3312",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install albumentations==0.4.5"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting albumentations==0.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/40/a343ecacc7e22fe52ab9a16b84dc6165ba05ee17e3729adeb3e2ffa2b37b/albumentations-0.4.5.tar.gz (116kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.5) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.5) (1.4.1)\n",
            "Collecting imgaug<0.2.7,>=0.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/2e/748dbb7bb52ec8667098bae9b585f448569ae520031932687761165419a2/imgaug-0.2.6.tar.gz (631kB)\n",
            "\u001b[K     |████████████████████████████████| 634kB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.5) (3.13)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.5) (4.1.2.30)\n",
            "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (0.16.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (1.15.0)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (3.2.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (2.5)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (1.1.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (7.0.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (2.8.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (4.4.2)\n",
            "Building wheels for collected packages: albumentations, imgaug\n",
            "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for albumentations: filename=albumentations-0.4.5-cp36-none-any.whl size=64378 sha256=8c8e8f40897bd88cbb07e871d650978cf57af050784f84990690d468371c802e\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/a0/61/e50f93165a5ec7e7f5d65064e513239505bc4c06d2289557d3\n",
            "  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imgaug: filename=imgaug-0.2.6-cp36-none-any.whl size=654021 sha256=80da34c441deec4d57ccc72f372d4e3b10093bcddf836ac8b326aca411be1c12\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/ec/48/0d25896c417b715af6236dbcef8f0bed136a1a5e52972fc6d0\n",
            "Successfully built albumentations imgaug\n",
            "Installing collected packages: imgaug, albumentations\n",
            "  Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "  Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-0.4.5 imgaug-0.2.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUJp0NcCW6sU",
        "outputId": "644682c8-806e-49a9-db1a-39b8bf0f321b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!cat /proc/cpuinfo | grep 'physical id' | sort | uniq | wc -l\n",
        "!cat /proc/cpuinfo |grep \"cores\"|uniq|awk '{print $4}'\n",
        "!grep 'processor' /proc/cpuinfo | sort -u | wc -l"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "20\n",
            "40\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRp7NhT6jVnG"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import os, sys, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import shutil\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torch import Tensor\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataloader import _utils\n",
        "\n",
        "from random import choice\n",
        "\n",
        "from skimage import io\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "import glob\n",
        "\n",
        "#from torchsummary import summary\n",
        "import logging\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.models as models\n",
        "# from tqdm.notebook import tqdm\n",
        "from tqdm import tqdm\n",
        "from sklearn.utils import shuffle\n",
        "# from apex import amp\n",
        "\n",
        "import random\n",
        "\n",
        "import time\n",
        "\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "from albumentations.augmentations.transforms import Lambda, ShiftScaleRotate, HorizontalFlip, Normalize, RandomBrightnessContrast, RandomResizedCrop\n",
        "from albumentations.pytorch import ToTensor\n",
        "from albumentations import Compose, OneOrOther"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhYIc93CvtbM",
        "outputId": "7a73c5dd-b601-44c5-af3b-d0fff36ffd34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "import albumentations\n",
        "albumentations.__version__"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0.4.5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfzNYK4-ZKNL"
      },
      "source": [
        "import warnings\n",
        "import torch_xla\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch_xla.distributed.data_parallel as dp\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.utils.utils as xu\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.test.test_utils as test_utils\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgZwl64lnnzk"
      },
      "source": [
        "def seed_everything(seed=1234):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    # torch.cuda.manual_seed(seed)\n",
        "    # torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNz7YC-Sq_4x"
      },
      "source": [
        "train_df = pd.read_csv(f'/content/drive/My Drive/BAA/train.csv')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzRygoRtwxkV",
        "outputId": "554b5fb7-a58b-4e43-c629-185535c0ba8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(train_df)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12611"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrIOGcpz6Fra",
        "outputId": "3f3bcf84-4528-4c7d-c2f4-ba145d8e8697",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        }
      },
      "source": [
        "plt.title('Male')\n",
        "train_df['male'].astype(int).hist()\n",
        "plt.show()\n",
        "plt.title('Female_age')\n",
        "train_df[train_df['male']==False]['boneage'].hist()\n",
        "plt.show()\n",
        "plt.title('Male_age')\n",
        "train_df[train_df['male']==True]['boneage'].hist()\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNWDSFBx9bJx"
      },
      "source": [
        "val_df = pd.read_csv(f'/content/drive/My Drive/BAA/Validation Dataset.csv')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6LindwLBR3d",
        "outputId": "939e3776-eefb-4fb8-a6df-f78fcc4430e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        }
      },
      "source": [
        "plt.title('Male')\n",
        "val_df['male'].astype(int).hist()\n",
        "plt.show()\n",
        "plt.title('Female_age')\n",
        "val_df[train_df['male']==False]['boneage'].hist()\n",
        "plt.show()\n",
        "plt.title('Male_age')\n",
        "val_df[train_df['male']==True]['boneage'].hist()\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdX9onshZ3pk"
      },
      "source": [
        "boneage_mean = train_df['boneage'].mean()\n",
        "boneage_div = train_df['boneage'].std()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp1JpnvvgYS5",
        "outputId": "f9d0c423-b072-44da-975b-9852d6d70376",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "train_df"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>boneage</th>\n",
              "      <th>male</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1377</td>\n",
              "      <td>180</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1378</td>\n",
              "      <td>12</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1379</td>\n",
              "      <td>94</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1380</td>\n",
              "      <td>120</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1381</td>\n",
              "      <td>82</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12606</th>\n",
              "      <td>15605</td>\n",
              "      <td>50</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12607</th>\n",
              "      <td>15606</td>\n",
              "      <td>113</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12608</th>\n",
              "      <td>15608</td>\n",
              "      <td>55</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12609</th>\n",
              "      <td>15609</td>\n",
              "      <td>150</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12610</th>\n",
              "      <td>15610</td>\n",
              "      <td>132</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12611 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  boneage   male\n",
              "0       1377      180  False\n",
              "1       1378       12  False\n",
              "2       1379       94  False\n",
              "3       1380      120   True\n",
              "4       1381       82  False\n",
              "...      ...      ...    ...\n",
              "12606  15605       50  False\n",
              "12607  15606      113  False\n",
              "12608  15608       55  False\n",
              "12609  15609      150   True\n",
              "12610  15610      132   True\n",
              "\n",
              "[12611 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWZVe55hgawx",
        "outputId": "cd2dd75a-53de-40d2-d096-3d4e2cb2762a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "val_df"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>male</th>\n",
              "      <th>boneage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1386</td>\n",
              "      <td>False</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1392</td>\n",
              "      <td>True</td>\n",
              "      <td>162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1397</td>\n",
              "      <td>False</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1401</td>\n",
              "      <td>False</td>\n",
              "      <td>132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1410</td>\n",
              "      <td>True</td>\n",
              "      <td>57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1420</th>\n",
              "      <td>15592</td>\n",
              "      <td>False</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1421</th>\n",
              "      <td>15601</td>\n",
              "      <td>False</td>\n",
              "      <td>132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1422</th>\n",
              "      <td>15607</td>\n",
              "      <td>True</td>\n",
              "      <td>186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1423</th>\n",
              "      <td>15611</td>\n",
              "      <td>False</td>\n",
              "      <td>120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1424</th>\n",
              "      <td>15612</td>\n",
              "      <td>True</td>\n",
              "      <td>132</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1425 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         id   male  boneage\n",
              "0      1386  False       30\n",
              "1      1392   True      162\n",
              "2      1397  False       18\n",
              "3      1401  False      132\n",
              "4      1410   True       57\n",
              "...     ...    ...      ...\n",
              "1420  15592  False       42\n",
              "1421  15601  False      132\n",
              "1422  15607   True      186\n",
              "1423  15611  False      120\n",
              "1424  15612   True      132\n",
              "\n",
              "[1425 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUavllAYP1LQ"
      },
      "source": [
        "test_df = pd.read_excel('/content/drive/My Drive/BAA/Bone age ground truth.xlsx')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlDFjLhZfhx-"
      },
      "source": [
        "### Create data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Juo73f72XhZT"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRfD97-YWBrw"
      },
      "source": [
        "# norm_mean = [0.485, 0.456, 0.406]\n",
        "# norm_std = [0.229, 0.224, 0.225]\n",
        "# #R*0.299 + G*0.587 + B*0.114\n",
        "# a = np.array([0.299, 0.587, 0.114])\n",
        "# a.dot(np.array(norm_mean))\n",
        "# a.dot(np.array(norm_std))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0B_59mX-7F0E"
      },
      "source": [
        "norm_mean = [0.143] #0.458971\n",
        "norm_std = [0.144] #0.225609\n",
        "\n",
        "# norm_mean = [0.143, 0.143, 0.143] \n",
        "# norm_std = [0.144, 0.144, 0.144] \n",
        "\n",
        "RandomErasing = transforms.RandomErasing(scale=(0.02, 0.08), ratio = (0.5, 2), p = 0.8)\n",
        "\n",
        "def randomErase(image, **kwargs):\n",
        "    return RandomErasing(image)\n",
        "\n",
        "def sample_normalize(image, **kwargs):\n",
        "    image = image/255\n",
        "    channel = image.shape[2]\n",
        "    mean, std = image.reshape((-1, channel)).mean(axis = 0), image.reshape((-1, channel)).std(axis = 0)\n",
        "    return (image-mean)/(std + 1e-3)\n",
        "\n",
        "\n",
        "# rotation range of 20 degrees, horizontal/vertical translation up to 20%, zoom up to 20% and a horizontal flip\n",
        "# transform_train = Compose([\n",
        "#     ShiftScaleRotate(shift_limit = 0.2, scale_limit = 0.2, rotate_limit=20, p = 0.8),\n",
        "#     HorizontalFlip(p = 0.5),\n",
        "#     RandomBrightnessContrast(p = 0.8), \n",
        "#     Normalize(mean=norm_mean, std = norm_std, p = 1),                                    \n",
        "#     ToTensor(),\n",
        "#     Lambda(image = randomErase) \n",
        "    \n",
        "# ])\n",
        "\n",
        "# transform_val = Compose([                                   \n",
        "#     Normalize(mean=norm_mean, std = norm_std, p = 1),\n",
        "#     ToTensor(),\n",
        "# ])\n",
        "\n",
        "transform_train = Compose([\n",
        "    # RandomBrightnessContrast(p = 0.8),\n",
        "    RandomResizedCrop(512, 512, (0.5, 1.0), p = 0.5),\n",
        "    ShiftScaleRotate(shift_limit = 0.2, scale_limit = 0.2, rotate_limit=20, border_mode = cv2.BORDER_CONSTANT, value = 0.0, p = 0.8),\n",
        "    # HorizontalFlip(p = 0.5),\n",
        "    \n",
        "    # ShiftScaleRotate(shift_limit = 0.2, scale_limit = 0.2, rotate_limit=20, p = 0.8),\n",
        "    HorizontalFlip(p = 0.5),\n",
        "    RandomBrightnessContrast(p = 0.8),                             \n",
        "    Lambda(image = sample_normalize),\n",
        "    ToTensor(),\n",
        "    Lambda(image = randomErase) \n",
        "    \n",
        "])\n",
        "\n",
        "transform_val = Compose([                                   \n",
        "    Lambda(image = sample_normalize),\n",
        "    ToTensor(),\n",
        "])\n",
        "\n",
        "transform_test = Compose([                                   \n",
        "    Lambda(image = sample_normalize),\n",
        "    ToTensor(),\n",
        "])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LE-OKIDiDOTk"
      },
      "source": [
        "# from torchvision.transforms import Normalize\n",
        "\n",
        "# class Unnormalize:\n",
        "#     \"\"\"Converts an image tensor that was previously Normalize'd\n",
        "#     back to an image with pixels in the range [0, 1].\"\"\"\n",
        "#     def __init__(self, mean, std):\n",
        "#         self.mean = mean\n",
        "#         self.std = std\n",
        "\n",
        "#     def __call__(self, tensor):\n",
        "#         mean = torch.as_tensor(self.mean, dtype=tensor.dtype, device=tensor.device)\n",
        "#         std = torch.as_tensor(self.std, dtype=tensor.dtype, device=tensor.device)\n",
        "#         return torch.clamp(tensor*std + mean, 0., 1.)\n",
        "\n",
        "# unnormalize_transform = Unnormalize(norm_mean, norm_std)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-CEpURmHCIL"
      },
      "source": [
        "# def read_image(path, image_size = 512):\n",
        "#     img = Image.open(path)\n",
        "#     w, h = img.size\n",
        "#     long = max(w, h)\n",
        "#     w, h = int(w/long*image_size), int(h/long*image_size)\n",
        "#     img = img.resize((w, h), Image.ANTIALIAS)\n",
        "#     delta_w, delta_h = image_size - w, image_size - h\n",
        "#     padding = (delta_w//2, delta_h//2, delta_w-(delta_w//2), delta_h-(delta_h//2))\n",
        "#     return np.expand_dims(np.array(ImageOps.expand(img, padding)), axis=2)\n",
        "\n",
        "def read_image(path, image_size = 512):\n",
        "    img = Image.open(path)\n",
        "    w, h = img.size\n",
        "    long = max(w, h)\n",
        "    w, h = int(w/long*image_size), int(h/long*image_size)\n",
        "    img = img.resize((w, h), Image.ANTIALIAS)\n",
        "    delta_w, delta_h = image_size - w, image_size - h\n",
        "    padding = (delta_w//2, delta_h//2, delta_w-(delta_w//2), delta_h-(delta_h//2))\n",
        "    return np.array(ImageOps.expand(img, padding).convert(\"RGB\"))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJN1OP-aCkLr"
      },
      "source": [
        "class BAATrainDataset(Dataset):\n",
        "    def __init__(self, df, file_path):\n",
        "        def preprocess_df(df):\n",
        "            #nomalize boneage distribution\n",
        "            df['zscore'] = df['boneage'].map(lambda x: (x - boneage_mean)/boneage_div )\n",
        "            #change the type of gender, change bool variable to float32\n",
        "            df['male'] = df['male'].astype('float32')\n",
        "            df['bonage'] = df['boneage'].astype('float32')\n",
        "            return df\n",
        "\n",
        "        self.df = preprocess_df(df)\n",
        "        self.file_path = file_path\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        row = self.df.iloc[index]\n",
        "        num = int(row['id'])\n",
        "        return (transform_train(image = read_image(f\"{self.file_path}/{num//1000}/{num}.png\"))['image'], Tensor([row['male']])), row['zscore']\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "class BAAValDataset(Dataset):\n",
        "    def __init__(self, df, file_path):\n",
        "        def preprocess_df(df):\n",
        "            #change the type of gender, change bool variable to float32\n",
        "            df['male'] = df['male'].astype('float32')\n",
        "            df['bonage'] = df['boneage'].astype('float32')\n",
        "            return df\n",
        "        self.df = preprocess_df(df)\n",
        "        self.file_path = file_path\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        row = self.df.iloc[index]\n",
        "        return (transform_val(image = read_image(f\"{self.file_path}/{int(row['id'])}.png\"))['image'], Tensor([row['male']])), row['boneage']\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "        \n",
        "class BAATestDataset(Dataset):\n",
        "    def __init__(self, df, file_path):\n",
        "        def preprocess_df(df):\n",
        "            #change the type of gender, change bool variable to float32\n",
        "            df['male'] = (df['Sex'] == 'M').astype('float32')\n",
        "            df['boneage'] = df['Ground truth bone age (months)'].astype('float32')\n",
        "            df['id'] = df['Case ID'].astype('int32')\n",
        "            return df\n",
        "        self.df = preprocess_df(df)\n",
        "        print(self.df.head())\n",
        "        self.file_path = file_path\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        row = self.df.iloc[index]\n",
        "        return (transform_test(image = read_image(f\"{self.file_path}/{int(row['id'])}.png\"))['image'], Tensor([row['male']])), row['boneage']\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df) \n",
        "\n",
        "def create_data_loader(train_df, val_df, test_df, train_root, val_root, test_root):\n",
        "    return BAATrainDataset(train_df, train_root), BAAValDataset(val_df, val_root), BAATestDataset(test_df, test_root)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JY9pq74GLBum",
        "outputId": "15ac1211-5444-453b-8766-6bdd287c1a13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls '/content/drive/My Drive/BAA/boneage-training-dataset/1377.png'"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access '/content/drive/My Drive/BAA/boneage-training-dataset/1377.png': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vA-p8KZcUM0M",
        "outputId": "773872a7-c364-46ad-8026-5ecd3d38f8c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_set, val_set, test_set = create_data_loader(train_df, val_df, test_df, '/content/drive/My Drive/BAA/boneage-training-dataset', '/content/drive/My Drive/BAA/boneage-validation-dataset', '/content/drive/My Drive/BAA/Test Set Images')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Case ID Sex  Ground truth bone age (months)  male     boneage    id\n",
            "0     4360   M                      168.934249   1.0  168.934250  4360\n",
            "1     4361   M                      169.652678   1.0  169.652679  4361\n",
            "2     4362   M                       73.256112   1.0   73.256111  4362\n",
            "3     4363   M                      152.862669   1.0  152.862671  4363\n",
            "4     4364   M                      135.456954   1.0  135.456955  4364\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9ZYChOKOHZc"
      },
      "source": [
        "# def create_data_loader(train_df, val_df, train_root, val_root, batch_size = 32, num_numbers = 4):\n",
        "#     train_set = DataLoader(BAATrainDataset(train_df, train_root), \\\n",
        "#                         batch_size = batch_size, pin_memory=True, num_workers = num_numbers)\n",
        "#     val_set = DataLoader(BAAValDataset(val_df, val_root),\\\n",
        "#                         batch_size = 2*batch_size, pin_memory=True, num_workers = num_numbers)\n",
        "#     return train_set, val_set\n",
        "\n",
        "# class Unnormalize:\n",
        "#     \"\"\"Converts an image tensor that was previously Normalize'd\n",
        "#     back to an image with pixels in the range [0, 1].\"\"\"\n",
        "#     def __init__(self, mean, std):\n",
        "#         self.mean = mean\n",
        "#         self.std = std\n",
        "\n",
        "#     def __call__(self, tensor):\n",
        "#         mean = torch.as_tensor(self.mean, dtype=tensor.dtype, device=tensor.device)\n",
        "#         std = torch.as_tensor(self.std, dtype=tensor.dtype, device=tensor.device)\n",
        "#         return torch.clamp(tensor*std + mean, 0., 1.)\n",
        "\n",
        "# unnormalize_transform = Unnormalize(norm_mean, norm_std)\n",
        "\n",
        "# train_loader, val_loader = create_data_loader(train_df, val_df, '/content/drive/My Drive/BAA/boneage-training-dataset', '/content/drive/My Drive/BAA/boneage-validation-dataset')\n",
        "# t_x, t_y = next(iter(train_loader))\n",
        "# fig, m_axs = plt.subplots(4, 4, figsize = (16, 8))\n",
        "\n",
        "# for (c_x0, c_x1, c_y, c_ax) in zip(*t_x, t_y, m_axs.flatten()):\n",
        "#     print(c_x0[0].shape)\n",
        "#     c_ax.imshow(unnormalize_transform(c_x0)[0], cmap = 'gray')\n",
        "#     c_ax.set_title(f'{(c_y*boneage_div+boneage_mean):2.0f} months, {\"male\" if c_x1 else \"female\"}')\n",
        "#     c_ax.axis('off')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ly2WDirlfo6P"
      },
      "source": [
        "### Define model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br4AbYctLaaN",
        "outputId": "5e37fe60-4984-4f80-e5f9-a452bebf2afd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "509b3f981dc64e1b823220f3e086468d",
            "d658b2a132974dbc96808654265c539b",
            "d1d38755703a4db4b2ced4f99fe5e5ed",
            "d188296bbc65413dbeab6a3d14a57614",
            "6c272e4d269f4af7b2db4af48ac3585f",
            "22e8ed338038412fba8f81f46ee21d22",
            "c2cb45bd8446493aa856387b0b04cae6",
            "1cb7b94a87f7451196bd97083d68edc0"
          ]
        }
      },
      "source": [
        "from pretrainedmodels import se_resnext101_32x4d, se_resnet152, xception, inceptionv4, inceptionresnetv2\n",
        "from torchvision.models import resnet18, resnet34, resnet50\n",
        "\n",
        "def get_My_resnet18():\n",
        "    model = resnet18(pretrained = True)\n",
        "    output_channels = model.fc.in_features\n",
        "    model = list(model.children())[:-2]\n",
        "    return model, output_channels\n",
        "\n",
        "def get_My_resnet34():\n",
        "    model = resnet34(pretrained = True)\n",
        "    output_channels = model.fc.in_features\n",
        "    model = list(model.children())[:-2]\n",
        "    return model, output_channels\n",
        "\n",
        "def get_My_resnet50():\n",
        "    model = resnet50(pretrained = True)\n",
        "    output_channels = model.fc.in_features\n",
        "    model = list(model.children())[:-2]\n",
        "    return model, output_channels\n",
        "\n",
        "def get_My_se_resnet152():\n",
        "    model = se_resnet152(pretrained = None)\n",
        "    output_channels = model.last_linear.in_features\n",
        "    model = nn.Sequential(*list(model.children())[:-2])\n",
        "    return model, output_channels\n",
        "\n",
        "def get_My_se_resnext101_32x4d():\n",
        "    model = se_resnext101_32x4d()\n",
        "    output_channels = model.last_linear.in_features\n",
        "    model = nn.Sequential(*list(model.children())[:-2])\n",
        "    return model, output_channels\n",
        "\n",
        "def get_My_inceptionv4():\n",
        "    model = inceptionv4()\n",
        "    output_channels = model.last_linear.in_features\n",
        "    model = list(model.children())[:-2]\n",
        "    \n",
        "    model = nn.Sequential(*model)\n",
        "    return model, output_channels\n",
        "\n",
        "def get_My_inceptionresnetv2():\n",
        "    model = inceptionresnetv2()\n",
        "    output_channels = model.last_linear.in_features\n",
        "    model = list(model.children())[:-2]\n",
        "    \n",
        "    model = nn.Sequential(*model)\n",
        "    return model, output_channels\n",
        "\n",
        "# get_My_se_resnet152()\n",
        "a, b = get_My_resnet34()\n",
        "a"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "509b3f981dc64e1b823220f3e086468d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=87306240.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False),\n",
              " BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
              " ReLU(inplace=True),\n",
              " MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False),\n",
              " Sequential(\n",
              "   (0): BasicBlock(\n",
              "     (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (relu): ReLU(inplace=True)\n",
              "     (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "   )\n",
              "   (1): BasicBlock(\n",
              "     (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (relu): ReLU(inplace=True)\n",
              "     (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "   )\n",
              "   (2): BasicBlock(\n",
              "     (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (relu): ReLU(inplace=True)\n",
              "     (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "   )\n",
              " ),\n",
              " Sequential(\n",
              "   (0): BasicBlock(\n",
              "     (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (relu): ReLU(inplace=True)\n",
              "     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (downsample): Sequential(\n",
              "       (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "       (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     )\n",
              "   )\n",
              "   (1): BasicBlock(\n",
              "     (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (relu): ReLU(inplace=True)\n",
              "     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "   )\n",
              "   (2): BasicBlock(\n",
              "     (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (relu): ReLU(inplace=True)\n",
              "     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "   )\n",
              "   (3): BasicBlock(\n",
              "     (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (relu): ReLU(inplace=True)\n",
              "     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "   )\n",
              " ),\n",
              " Sequential(\n",
              "   (0): BasicBlock(\n",
              "     (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (relu): ReLU(inplace=True)\n",
              "     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (downsample): Sequential(\n",
              "       (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "       (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     )\n",
              "   )\n",
              "   (1): BasicBlock(\n",
              "     (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (relu): ReLU(inplace=True)\n",
              "     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "   )\n",
              "   (2): BasicBlock(\n",
              "     (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (relu): ReLU(inplace=True)\n",
              "     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "   )\n",
              "   (3): BasicBlock(\n",
              "     (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (relu): ReLU(inplace=True)\n",
              "     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "   )\n",
              "   (4): BasicBlock(\n",
              "     (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (relu): ReLU(inplace=True)\n",
              "     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "   )\n",
              "   (5): BasicBlock(\n",
              "     (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (relu): ReLU(inplace=True)\n",
              "     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "   )\n",
              " ),\n",
              " Sequential(\n",
              "   (0): BasicBlock(\n",
              "     (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (relu): ReLU(inplace=True)\n",
              "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (downsample): Sequential(\n",
              "       (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "       (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     )\n",
              "   )\n",
              "   (1): BasicBlock(\n",
              "     (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (relu): ReLU(inplace=True)\n",
              "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "   )\n",
              "   (2): BasicBlock(\n",
              "     (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (relu): ReLU(inplace=True)\n",
              "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "   )\n",
              " )]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zymiOIfXeirt"
      },
      "source": [
        "# from collections import OrderedDict\n",
        "# layer0_modules = [\n",
        "#   ('conv1', nn.Conv2d(1, 64, kernel_size=7, stride=2,\n",
        "#                       padding=3, bias=False)),\n",
        "#   ('bn1', nn.BatchNorm2d(64)),\n",
        "#   ('relu1', nn.ReLU(inplace=True)),\n",
        "# ]\n",
        "# layer0_modules.append(('pool', nn.MaxPool2d(3, stride=2, ceil_mode=True)))\n",
        "# layer0_modules = nn.Sequential(OrderedDict(layer0_modules))\n",
        "\n",
        "# from pretrainedmodels import se_resnext101_32x4d, se_resnet152, xception, inceptionv4\n",
        "\n",
        "# def get_My_se_resnet152():\n",
        "#     model = se_resnet152()\n",
        "#     output_channels = model.last_linear.in_features\n",
        "#     model = list(model.children())[:-2]\n",
        "#     model[0] = layer0_modules\n",
        "    \n",
        "#     model = nn.Sequential(*model)\n",
        "#     return model, output_channels\n",
        "\n",
        "# def get_My_xception():\n",
        "#     model = xception()\n",
        "#     output_channels = model.last_linear.in_features\n",
        "#     model = list(model.children())[:-2]\n",
        "#     model[0] = nn.Conv2d(1, 32, 3,2, 0, bias=False)\n",
        "    \n",
        "#     model = nn.Sequential(*model)\n",
        "#     return model, output_channels\n",
        "\n",
        "# class BasicConv2d(nn.Module):\n",
        "\n",
        "#     def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):\n",
        "#         super(BasicConv2d, self).__init__()\n",
        "#         self.conv = nn.Conv2d(in_planes, out_planes,\n",
        "#                               kernel_size=kernel_size, stride=stride,\n",
        "#                               padding=padding, bias=False) # verify bias false\n",
        "#         self.bn = nn.BatchNorm2d(out_planes,\n",
        "#                                  eps=0.001, # value found in tensorflow\n",
        "#                                  momentum=0.1, # default pytorch value\n",
        "#                                  affine=True)\n",
        "#         self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.conv(x)\n",
        "#         x = self.bn(x)\n",
        "#         x = self.relu(x)\n",
        "#         return x\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuZHE7Sl-CRW",
        "outputId": "5bf3acbd-d4f9-4578-b821-3c98cce74072",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "4//2**2"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIGDLj8Rg86W"
      },
      "source": [
        "class Pooling_attention(nn.Module):\n",
        "  def __init__(self, input_channels, kernel_size = 1):\n",
        "    super(Pooling_attention, self).__init__()\n",
        "    self.pooling_attention = nn.Sequential(\n",
        "        nn.Conv2d(input_channels, 1, kernel_size = kernel_size, padding = kernel_size//2),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    return self.pooling_attention(x) \n",
        "\n",
        "\n",
        "# class Part_Relation(nn.Module):\n",
        "#   def __init__(self, input_channels, reduction = 16):\n",
        "#     super(Part_Relation, self).__init__()\n",
        "#     self.pooling_attention_0 = nn.Sequential(\n",
        "#       nn.Conv2d(input_channels, input_channels//reduction, kernel_size = 1),\n",
        "#       nn.BatchNorm2d(input_channels//reduction),\n",
        "#       nn.ReLU()\n",
        "#     )\n",
        "#     self.pooling_attention_1 = Pooling_attention(input_channels//reduction, 1)\n",
        "#     self.pooling_attention_3 = Pooling_attention(input_channels//reduction, 3)\n",
        "#     self.pooling_attention_5 = Pooling_attention(input_channels//reduction, 5)\n",
        "\n",
        "#     self.last_conv =  nn.Sequential(\n",
        "#         nn.Conv2d(3, 1, kernel_size = 1),\n",
        "#         nn.Sigmoid()\n",
        "#     )\n",
        "\n",
        "\n",
        "    # self.se = SEModule(input_channels, 16)\n",
        "#   def forward(self, x):\n",
        "#     input = x\n",
        "#     x = self.pooling_attention_0(x)\n",
        "#     x = torch.cat([self.pooling_attention_1(x), self.pooling_attention_3(x), self.pooling_attention_5(x)], dim = 1)\n",
        "#     x = self.last_conv(x)\n",
        "#     return input + input*x\n",
        "\n",
        "class Part_Relation(nn.Module):\n",
        "  def __init__(self, input_channels, reduction = [16], level = 1):\n",
        "    super(Part_Relation, self).__init__()\n",
        "    \n",
        "    modules = []\n",
        "    for i in range(level):\n",
        "        output_channels = input_channels//reduction[i]\n",
        "        modules.append(nn.Conv2d(input_channels, output_channels, kernel_size = 1))\n",
        "        modules.append(nn.BatchNorm2d(output_channels))\n",
        "        modules.append(nn.ReLU())\n",
        "        input_channels = output_channels\n",
        "\n",
        "    self.pooling_attention_0 = nn.Sequential(*modules)\n",
        "    self.pooling_attention_1 = Pooling_attention(input_channels, 1)\n",
        "    self.pooling_attention_3 = Pooling_attention(input_channels, 3)\n",
        "    self.pooling_attention_5 = Pooling_attention(input_channels, 5)\n",
        "\n",
        "    self.last_conv =  nn.Sequential(\n",
        "        nn.Conv2d(3, 1, kernel_size = 1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    input = x\n",
        "    x = self.pooling_attention_0(x)\n",
        "    x = torch.cat([self.pooling_attention_1(x), self.pooling_attention_3(x), self.pooling_attention_5(x)], dim = 1)\n",
        "    x = self.last_conv(x)\n",
        "    return input*x"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nupnBC96r54k"
      },
      "source": [
        "class BAA_New(nn.Module):\n",
        "    def __init__(self, gender_encode_length, backbone, out_channels):\n",
        "        super(BAA_New, self).__init__()\n",
        "        self.backbone0 = nn.Sequential(*backbone[0:5])\n",
        "        self.out_channels = out_channels\n",
        "        self.backbone1 = backbone[5]\n",
        "        self.backbone2 = backbone[6]\n",
        "        self.backbone3 = backbone[7]\n",
        "\n",
        "        self.part_relation0 = Part_Relation(64)\n",
        "        self.part_relation1 = Part_Relation(128)\n",
        "        self.part_relation2 = Part_Relation(256)\n",
        "        self.part_relation3 = Part_Relation(512)\n",
        "\n",
        "        self.gender_encoder = nn.Linear(1, gender_encode_length)\n",
        "        self.gender_bn = nn.BatchNorm1d(gender_encode_length)\n",
        "\n",
        "        self.fc0 = nn.Linear(out_channels + gender_encode_length, 512)\n",
        "        self.bn0 = nn.BatchNorm1d(512)\n",
        "\n",
        "        self.fc1 = nn.Linear(512, 256)\n",
        "        self.bn1 = nn.BatchNorm1d(256)\n",
        "\n",
        "        self.output = nn.Linear(256, 1)\n",
        "\n",
        "    def forward(self, image, gender):\n",
        "        x = self.part_relation0(self.backbone0(image))\n",
        "        # x  = self.backbone0(image)\n",
        "        x = self.part_relation1(self.backbone1(x))\n",
        "        # x = self.backbone1(x)\n",
        "        x = self.part_relation2(self.backbone2(x))\n",
        "        # x = self.backbone2(x)\n",
        "        x = self.part_relation3(self.backbone3(x))\n",
        "        # x = self.backbone3(x)\n",
        "\n",
        "        x = F.adaptive_avg_pool2d(x, 1)\n",
        "        x = torch.squeeze(x)\n",
        "        x.view(-1, self.out_channels)\n",
        "\n",
        "        gender_encode = self.gender_bn(self.gender_encoder(gender))\n",
        "        gender_encode = F.relu(gender_encode)\n",
        "\n",
        "        x = torch.cat([x,  gender_encode], dim = 1)\n",
        "\n",
        "        x = F.relu(self.bn0(self.fc0(x)))\n",
        "\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "\n",
        "        return self.output(x)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fAgezHYFECo"
      },
      "source": [
        "# class Part_Relation(nn.Module):\n",
        "#   def __init__(self, input_channels, kernel_size = 1):\n",
        "#     super(Part_Relation, self).__init__()\n",
        "#     self.pooling_attention = nn.Sequential(\n",
        "#       nn.Conv2d(input_channels, 1, kernel_size = kernel_size, padding = kernel_size//2),\n",
        "#       nn.Sigmoid()\n",
        "#     )\n",
        "#   def forward(self, x):\n",
        "#     return x + x*self.pooling_attention(x)\n",
        "\n",
        "\n",
        "# class BAA_New(nn.Module):\n",
        "#     def __init__(self, gender_encode_length, backbone, out_channels):\n",
        "#         super(BAA_New, self).__init__()\n",
        "#         self.backbone0 = nn.Sequential(*backbone[0:5])\n",
        "#         self.out_channels = out_channels\n",
        "#         self.backbone1 = backbone[5]\n",
        "#         self.backbone2 = backbone[6]\n",
        "#         self.backbone3 = backbone[7]\n",
        "\n",
        "#         self.part_relation0 = Part_Relation(64, 1)\n",
        "#         self.part_relation1 = Part_Relation(128,1)\n",
        "#         self.part_relation2 = Part_Relation(256, 1)\n",
        "#         self.part_relation3 = Part_Relation(512)\n",
        "\n",
        "#         self.gender_encoder = nn.Linear(1, gender_encode_length)\n",
        "#         self.gender_bn = nn.BatchNorm1d(gender_encode_length)\n",
        "\n",
        "#         self.fc0 = nn.Linear(out_channels + gender_encode_length, 512)\n",
        "#         self.bn0 = nn.BatchNorm1d(512)\n",
        "\n",
        "#         self.fc1 = nn.Linear(512, 256)\n",
        "#         self.bn1 = nn.BatchNorm1d(256)\n",
        "\n",
        "#         self.output = nn.Linear(256, 1)\n",
        "\n",
        "#     def forward(self, image, gender):\n",
        "#         x = self.backbone0(image)\n",
        "#         # x  = self.backbone0(image)\n",
        "#         x = self.backbone1(x)\n",
        "#         # x = self.backbone1(x)\n",
        "#         x = self.backbone2(x)\n",
        "#         x = self.part_relation3(self.backbone3(x))\n",
        "\n",
        "#         x = F.adaptive_avg_pool2d(x, 1)\n",
        "#         x = torch.squeeze(x)\n",
        "#         x.view(-1, self.out_channels)\n",
        "\n",
        "#         gender_encode = self.gender_bn(self.gender_encoder(gender))\n",
        "#         gender_encode = F.relu(gender_encode)\n",
        "\n",
        "#         x = torch.cat([x,  gender_encode], dim = 1)\n",
        "\n",
        "#         x = F.relu(self.bn0(self.fc0(x)))\n",
        "\n",
        "#         x = F.relu(self.bn1(self.fc1(x)))\n",
        "\n",
        "        # return self.output(x)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-0L8DnRy5kb"
      },
      "source": [
        "# class BAA_base1(nn.Module):\n",
        "#     def __init__(self, gender_encode_length, backbone, out_channels):\n",
        "#         super(BAA_base1, self).__init__()\n",
        "#         self.backbone = backbone\n",
        "        \n",
        "#         self.gender_encoder = nn.Linear(1, gender_encode_length)\n",
        "#         self.gender_bn = nn.BatchNorm1d(gender_encode_length)\n",
        "\n",
        "#         self.fc0 = nn.Linear(out_channels + gender_encode_length, 512)\n",
        "#         self.bn0 = nn.BatchNorm1d(512)\n",
        "\n",
        "#         self.dropout = nn.Dropout(p = 0.2)\n",
        "\n",
        "#         self.fc1 = nn.Linear(512, 256)\n",
        "#         self.bn1 = nn.BatchNorm1d(256)\n",
        "\n",
        "#         self.fc2 = nn.Linear(256, 128)\n",
        "#         self.bn2 = nn.BatchNorm1d(128)\n",
        "\n",
        "#         self.output = nn.Linear(128, 1)\n",
        "\n",
        "#     def forward(self, image, gender):\n",
        "#         image_encode = F.adaptive_avg_pool2d(self.backbone(image), 1)\n",
        "#         image_encode = torch.squeeze(image_encode)\n",
        "\n",
        "#         gender_encode = self.gender_bn(self.gender_encoder(gender))\n",
        "#         gender_encode = F.relu(gender_encode)\n",
        "\n",
        "#         x = torch.cat([image_encode,  gender_encode], dim = 1)\n",
        "\n",
        "#         x = F.relu(self.bn0(self.fc0(x)))\n",
        "\n",
        "#         x = F.relu(self.bn1(self.fc1(x)))\n",
        "\n",
        "#         x = F.relu(self.bn2(self.fc2(x)))\n",
        "\n",
        "#         # x = self.dropout(x)\n",
        "\n",
        "#         return self.output(x)\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inXABwAsyBol"
      },
      "source": [
        "# model = BAA_base1(32, *get_My_se_resnet152())\n",
        "# model.load_state_dict(torch.load('/content/drive/My Drive/trained_net/senet152_fc_512_256_128/best_senet152_fc_512_256_128.bin'))\n",
        "# model = list(model.children())[0]"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgO5b2hk4TSL"
      },
      "source": [
        "# class BAA_base(nn.Module):\n",
        "#     def __init__(self, gender_encode_length):\n",
        "#         super(BAA_base, self).__init__()\n",
        "#         self.backbone, out_channels = model, 2048\n",
        "#         self.conv_pooling = nn.Conv1d(2048, 1024, kernel_size = 2)\n",
        "        \n",
        "#         self.gender_encoder = nn.Linear(1, gender_encode_length)\n",
        "#         self.gender_bn = nn.BatchNorm1d(gender_encode_length)\n",
        "\n",
        "#         self.fc0 = nn.Linear(1024 + gender_encode_length, 512)\n",
        "#         self.bn0 = nn.BatchNorm1d(512)\n",
        "\n",
        "#         self.dropout = nn.Dropout(p = 0.2)\n",
        "\n",
        "#         self.fc1 = nn.Linear(512, 256)\n",
        "#         self.bn1 = nn.BatchNorm1d(256)\n",
        "\n",
        "#         self.fc2 = nn.Linear(256, 128)\n",
        "#         self.bn2 = nn.BatchNorm1d(128)\n",
        "\n",
        "#         self.output = nn.Linear(128, 1)\n",
        "\n",
        "#     def forward(self, image, gender):\n",
        "#         image_encode = self.backbone(image)\n",
        "        \n",
        "#         image_encode = torch.squeeze(image_encode)\n",
        "#         image_encode = self.conv_pooling(image_encode)\n",
        "#         image_encode = torch.squeeze(image_encode)\n",
        "\n",
        "#         gender_encode = self.gender_bn(self.gender_encoder(gender))\n",
        "#         gender_encode = F.relu(gender_encode)\n",
        "\n",
        "#         x = torch.cat([image_encode,  gender_encode], dim = 1)\n",
        "\n",
        "#         x = F.relu(self.bn0(self.fc0(x)))\n",
        "\n",
        "#         x = F.relu(self.bn1(self.fc1(x)))\n",
        "\n",
        "#         x = F.relu(self.bn2(self.fc2(x)))\n",
        "\n",
        "#         # x = self.dropout(x)\n",
        "\n",
        "#         return self.output(x)\n",
        "# BAA_base(32)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-ZAzULWjFDa"
      },
      "source": [
        "# model = BAA_base(32)\n",
        "# model.load_state_dict(torch.load('/content/drive/My Drive/trained_net/senet152_fc_512_256_128/best_senet152_fc_512_256_128.bin'))\n",
        "# model = list(model.children())[0]\n",
        "# model[4]"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gaJzKE0SV4-"
      },
      "source": [
        "# from copy import deepcopy\n",
        "# class BAA_New(nn.Module):\n",
        "#   def __init__(self, model):\n",
        "#     super(BAA_New, self).__init__()\n",
        "\n",
        "#     self.encoder = nn.Sequential(*model[:-1])\n",
        "\n",
        "#     self.male = deepcopy(model[-1])\n",
        "#     self.female = deepcopy(model[-1])\n",
        "\n",
        "#     self.fc0 = nn.Linear(2048, 1024)\n",
        "#     self.bn0 = nn.BatchNorm1d(1024)\n",
        "\n",
        "#     # self.fc1 = nn.Linear(1024, 1024)\n",
        "#     # self.bn1 = nn.BatchNorm1d(1024)\n",
        "\n",
        "#     self.output = nn.Linear(1024, 1)\n",
        "\n",
        "#   def forward(self, image, gender):\n",
        "#     x = self.encoder(image)\n",
        "#     gender = gender.view(-1 ,1, 1, 1)\n",
        "#     x = self.female(x)*gender + self.male(x)*(1 - gender)\n",
        "#     x = F.adaptive_avg_pool2d(x, 1)\n",
        "#     x = torch.squeeze(x)\n",
        "#     x = F.relu(self.bn0(self.fc0(x)))\n",
        "#     # x = F.relu(self.bn1(self.fc1(x)))\n",
        "#     return self.output(x)\n",
        "\n",
        "\n",
        "# BAA_New(model)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQw41I9Gno0I"
      },
      "source": [
        "# from copy import deepcopy\n",
        "# class BAA_New(nn.Module):\n",
        "#   def __init__(self, model):\n",
        "#     super(BAA_New, self).__init__()\n",
        "\n",
        "#     self.encoder = nn.Sequential(*model)\n",
        "\n",
        "\n",
        "#     self.fc0 = nn.Linear(2048, 1024)\n",
        "#     self.bn0 = nn.BatchNorm1d(1024)\n",
        "\n",
        "#     self.fc1 = nn.Linear(1024, 1024)\n",
        "#     self.bn1 = nn.BatchNorm1d(1024)\n",
        "\n",
        "#     self.output = nn.Linear(1024, 1)\n",
        "\n",
        "#   def forward(self, image, gender):\n",
        "#     x = self.encoder(image)\n",
        "    \n",
        "#     x = F.adaptive_avg_pool2d(x, 1)\n",
        "#     x = F.relu(self.bn0(self.fc0(x)))\n",
        "#     x = F.relu(self.bn1(self.fc1(x)))\n",
        "\n",
        "#     return self.output(x)\n",
        "\n",
        "\n",
        "# BAA_New(model)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l77tmD4anLxU"
      },
      "source": [
        "# class BAA_base(nn.Module):\n",
        "#     def __init__(self, gender_encode_length):\n",
        "#         super(BAA_base, self).__init__()\n",
        "#         self.backbone, out_channels = get_My_se_resnet152()\n",
        "        \n",
        "        \n",
        "#         self.gender_encoder = nn.Linear(1, gender_encode_length)\n",
        "#         self.gender_bn = nn.BatchNorm1d(gender_encode_length)\n",
        "\n",
        "#         self.fc_male = nn.Linear(out_channels, gender_encode_length)\n",
        "#         self.bn_male = nn.BatchNorm1d(gender_encode_length)\n",
        "\n",
        "#         self.fc_female = nn.Linear(out_channels, gender_encode_length)\n",
        "#         self.bn_female = nn.BatchNorm1d(gender_encode_length)\n",
        "\n",
        "#         self.dropout = nn.Dropout()\n",
        "\n",
        "#         self.fc0 = nn.Linear(out_channels + gender_encode_length, 1024)\n",
        "#         self.bn0 = nn.BatchNorm1d(1024)\n",
        "\n",
        "#         self.fc1 = nn.Linear(1024, 1024)\n",
        "#         self.bn1 = nn.BatchNorm1d(1024)\n",
        "#         self.output = nn.Linear(1024, 1)\n",
        "\n",
        "#     def forward(self, image, gender):\n",
        "#         image_encode = F.adaptive_avg_pool2d(self.backbone(image), 1)\n",
        "#         image_encode = torch.squeeze(image_encode)\n",
        "\n",
        "#         male = F.relu(self.bn_male(self.fc_male(image_encode)))\n",
        "#         female = F.relu(self.bn_female(self.fc_female(image_encode)))\n",
        "\n",
        "#         g = gender*male + (1 - gender)*female\n",
        "\n",
        "#         x = torch.cat([g, image_encode], dim = 1)\n",
        "\n",
        "#         x = F.relu(self.bn0(self.fc0(x)))\n",
        "\n",
        "#         x = F.relu(self.bn1(self.fc1(x)))\n",
        "\n",
        "#         # x = self.dropout(x)\n",
        "\n",
        "#         return self.output(x)\n",
        "# BAA_base(32)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrSkhLpm_i7O"
      },
      "source": [
        "# def train_fn(net, train_loader, loss_fn, epoch, optimizer, device):\n",
        "#     '''\n",
        "#     checkpoint is a dict\n",
        "#     '''\n",
        "#     global total_size \n",
        "#     global training_loss \n",
        "\n",
        "#     batch_accumulator = 2\n",
        "#     backprop_flag = False\n",
        "#     net.train()\n",
        "#     if xm.is_master_ordinal():\n",
        "#         train_pbar = tqdm(train_loader)\n",
        "#         train_pbar.desc = f'Epoch {epoch + 1}'\n",
        "#     else:\n",
        "#         train_pbar = train_loader\n",
        "\n",
        "#     optimizer.zero_grad()\n",
        "#     for batch_idx, data in enumerate(train_pbar):\n",
        "#         # #put data to GPU\n",
        "#         backprop_flag = False\n",
        "#         size = len(data[1])\n",
        "        \n",
        "#         image, gender = data[0]\n",
        "#         image, gender= image.to(device), gender.to(device)\n",
        "\n",
        "#         label = data[1].to(device)\n",
        "\n",
        "#         batch_size = len(data[1])\n",
        "#         label = data[1].to(device)\n",
        "        \n",
        "#         #forward\n",
        "#         y_pred = net(image, gender)\n",
        "#         y_pred = y_pred.squeeze()\n",
        "\n",
        "#         # print(y_pred, label)\n",
        "#         loss = loss_fn(y_pred, label)\n",
        "#         #backward,calculate gradients\n",
        "#         loss.backward()\n",
        "\n",
        "#         if batch_idx%batch_accumulator == 1:\n",
        "#             backprop_flag = True\n",
        "            \n",
        "#             #backward,update parameter\n",
        "#             xm.optimizer_step(optimizer)\n",
        "#             # zero the parameter gradients\n",
        "#             optimizer.zero_grad()\n",
        "            \n",
        "#         else:\n",
        "#             xm.rendezvous('123')\n",
        "\n",
        "#         #the learning rate should be update after optimizer's update \n",
        "#         #change the learning rate, because using One cycle pollicy,the learning rate should be update per mini-batch\n",
        "#         # scheduler.step()\n",
        "\n",
        "#         batch_loss = loss.item()\n",
        "\n",
        "#         training_loss += batch_loss\n",
        "#         total_size += batch_size\n",
        "#         if xm.is_master_ordinal():\n",
        "#             train_pbar.set_postfix({'loss': batch_loss/batch_size})\n",
        "#     else:\n",
        "#         if backprop_flag == False:\n",
        "#             #backward,update parameter\n",
        "#             xm.optimizer_step(optimizer)\n",
        "#             # zero the parameter gradients\n",
        "#             optimizer.zero_grad()\n",
        "\n",
        "\n",
        "#     return training_loss/total_size "
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNMQTCcURnKu"
      },
      "source": [
        "def train_fn(net, train_loader, loss_fn, epoch, optimizer, device):\n",
        "    '''\n",
        "    checkpoint is a dict\n",
        "    '''\n",
        "    global total_size \n",
        "    global training_loss \n",
        "\n",
        "    net.train()\n",
        "    if xm.is_master_ordinal():\n",
        "        train_pbar = tqdm(train_loader)\n",
        "        train_pbar.desc = f'Epoch {epoch + 1}'\n",
        "    else:\n",
        "        train_pbar = train_loader\n",
        "    for batch_idx, data in enumerate(train_pbar):\n",
        "        # #put data to GPU\n",
        "        size = len(data[1])\n",
        "        \n",
        "        image, gender = data[0]\n",
        "        image, gender= image.to(device), gender.to(device)\n",
        "\n",
        "        label = data[1].to(device)\n",
        "\n",
        "        batch_size = len(data[1])\n",
        "        label = data[1].to(device)\n",
        "        \n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        #forward\n",
        "        y_pred = net(image, gender)\n",
        "        y_pred = y_pred.squeeze()\n",
        "\n",
        "        # print(y_pred, label)\n",
        "        loss = loss_fn(y_pred, label)\n",
        "        #backward,calculate gradients\n",
        "        loss.backward()\n",
        "        #backward,update parameter\n",
        "        xm.optimizer_step(optimizer)\n",
        "\n",
        "        #the learning rate should be update after optimizer's update \n",
        "        #change the learning rate, because using One cycle pollicy,the learning rate should be update per mini-batch\n",
        "        # scheduler.step()\n",
        "\n",
        "        batch_loss = loss.item()\n",
        "\n",
        "        training_loss += batch_loss\n",
        "        total_size += batch_size\n",
        "        if xm.is_master_ordinal():\n",
        "            train_pbar.set_postfix({'loss': batch_loss/batch_size})\n",
        "        # print('loss:', batch_loss/batch_size)\n",
        "        # print(f'xla:{xm.get_ordinal()}, batch is{batch_idx}, loss is {mse_loss/total_size}, {size}')\n",
        "    return training_loss/total_size "
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5M8QF3ahu9K"
      },
      "source": [
        "def evaluate_fn(net, val_loader, device):\n",
        "    net.train(False)\n",
        "    \n",
        "    global mae_loss \n",
        "    global val_total_size \n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data in enumerate(val_loader):\n",
        "            val_total_size += len(data[1])\n",
        "\n",
        "            image, gender = data[0]\n",
        "            image, gender= image.to(device), gender.to(device)\n",
        "\n",
        "            label = data[1].to(device)\n",
        "\n",
        "            y_pred = net(image, gender)*boneage_div+boneage_mean\n",
        "            # y_pred = net(image, gender)\n",
        "            y_pred = y_pred.squeeze()\n",
        "\n",
        "            batch_loss = F.l1_loss(y_pred, label, reduction='sum').item()\n",
        "            # print(batch_loss/len(data[1]))\n",
        "            mae_loss += batch_loss\n",
        "    return mae_loss\n",
        "\n",
        "def test_fn(net, test_loader, device):\n",
        "    net.train(False)\n",
        "    \n",
        "    global test_mae_loss \n",
        "    global test_total_size \n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data in enumerate(test_loader):\n",
        "            test_total_size += len(data[1])\n",
        "\n",
        "            image, gender = data[0]\n",
        "            image, gender= image.to(device), gender.to(device)\n",
        "\n",
        "            label = data[1].to(device)\n",
        "\n",
        "            y_pred = net(image, gender)*boneage_div+boneage_mean\n",
        "            # y_pred = net(image, gender)\n",
        "            y_pred = y_pred.squeeze()\n",
        "\n",
        "            batch_loss = F.l1_loss(y_pred, label, reduction='sum').item()\n",
        "            # print(batch_loss/len(data[1]))\n",
        "            test_mae_loss += batch_loss\n",
        "    return mae_loss"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obFL4BVuhLpw"
      },
      "source": [
        "def reduce_fn(vals):\n",
        "    return sum(vals)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3t81bdBGDPRB"
      },
      "source": [
        "import torchvision\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import time\n",
        "\n",
        "def map_fn(index, flags):\n",
        "\n",
        "  ## Setup\n",
        "  root = '/content/drive/My Drive/BAA'\n",
        "  model_name = 'msa34' \n",
        "  path = f'{root}/{model_name}'\n",
        "\n",
        "  if xm.is_master_ordinal():\n",
        "    if not os.path.exists(path):\n",
        "        os.mkdir(path)\n",
        "        \n",
        "  # Sets a common random seed - both for initialization and ensuring graph is the same\n",
        "  seed_everything(seed=flags['seed'])\n",
        "\n",
        "  # Acquires the (unique) Cloud TPU core corresponding to this process's index\n",
        "  device = xm.xla_device()\n",
        "\n",
        "\n",
        "#   mymodel = BAA_base(32)\n",
        "  mymodel = BAA_New(32, *get_My_resnet34())\n",
        "#   mymodel.load_state_dict(torch.load('/content/drive/My Drive/BAA/resnet50_pr_2/best_resnet50_pr_2.bin'))\n",
        "  mymodel = mymodel.to(device)\n",
        "  \n",
        "  # Creates the (distributed) train sampler, which let this process only access\n",
        "  # its portion of the training dataset.\n",
        "  train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    train_set,\n",
        "    num_replicas=xm.xrt_world_size(),\n",
        "    rank=xm.get_ordinal(),\n",
        "    shuffle=True)\n",
        "  \n",
        "  val_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    val_set,\n",
        "    num_replicas=xm.xrt_world_size(),\n",
        "    rank=xm.get_ordinal(),\n",
        "    shuffle=False)\n",
        "  \n",
        "  test_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    test_set,\n",
        "    num_replicas=xm.xrt_world_size(),\n",
        "    rank=xm.get_ordinal(),\n",
        "    shuffle=False)\n",
        "  \n",
        "  # Creates dataloaders, which load data in batches\n",
        "  # Note: test loader is not shuffled or sampled\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "      train_set,\n",
        "      batch_size=flags['batch_size'],\n",
        "      sampler=train_sampler,\n",
        "      num_workers=flags['num_workers'],\n",
        "      drop_last=True)\n",
        "\n",
        "  val_loader = torch.utils.data.DataLoader(\n",
        "      val_set,\n",
        "      batch_size=flags['batch_size'],\n",
        "      sampler=val_sampler,\n",
        "      shuffle=False,\n",
        "      num_workers=flags['num_workers'])\n",
        "  \n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "      test_set,\n",
        "      batch_size=flags['batch_size'],\n",
        "      sampler=test_sampler,\n",
        "      shuffle=False,\n",
        "      num_workers=flags['num_workers'])  \n",
        "\n",
        "  ## Network, optimizer, and loss function creation\n",
        "\n",
        "  # Creates AlexNet for 10 classes\n",
        "  # Note: each process has its own identical copy of the model\n",
        "  #  Even though each model is created independently, they're also\n",
        "  #  created in the same way.\n",
        "  net = mymodel.train()\n",
        "\n",
        "  global best_loss \n",
        "  best_loss = float('inf')\n",
        "#   loss_fn =  nn.MSELoss(reduction = 'sum')\n",
        "  loss_fn = nn.L1Loss(reduction = 'sum')\n",
        "  lr = flags['lr']\n",
        "\n",
        "  wd = 0\n",
        "    \n",
        "  optimizer = torch.optim.Adam(mymodel.parameters(), lr=lr, weight_decay=wd)\n",
        "#   optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay = wd)\n",
        "  scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "  ## Trains\n",
        "  train_start = time.time()\n",
        "  for epoch in range(flags['num_epochs']):\n",
        "    global training_loss \n",
        "    training_loss = torch.tensor([0], dtype = torch.float32)\n",
        "    global total_size \n",
        "    total_size = torch.tensor([0], dtype = torch.float32)\n",
        "\n",
        "    global mae_loss \n",
        "    mae_loss = torch.tensor([0], dtype = torch.float32)\n",
        "    global val_total_size \n",
        "    val_total_size = torch.tensor([0], dtype = torch.float32)\n",
        "\n",
        "    global test_mae_loss \n",
        "    test_mae_loss = torch.tensor([0], dtype = torch.float32)\n",
        "    global test_total_size \n",
        "    test_total_size = torch.tensor([0], dtype = torch.float32)\n",
        "    # xm.rendezvous(\"initialization\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    para_train_loader = pl.ParallelLoader(train_loader, [device]).per_device_loader(device)\n",
        "    train_fn(net, para_train_loader, loss_fn, epoch, optimizer, device)\n",
        "    \n",
        "    ## Evaluation\n",
        "    # Sets net to eval and no grad context\n",
        "    para_val_loader = pl.ParallelLoader(val_loader, [device]).per_device_loader(device)\n",
        "    evaluate_fn(net, para_val_loader, device)\n",
        "\n",
        "    para_test_loader = pl.ParallelLoader(test_loader, [device]).per_device_loader(device)\n",
        "    test_fn(net, para_test_loader, device)\n",
        "\n",
        "    scheduler.step()\n",
        "    \n",
        "    xm.save(net.state_dict(), '/'.join([path, f'{model_name}.bin']))  \n",
        "    training_loss = xm.mesh_reduce('training_loss',training_loss,reduce_fn)\n",
        "    total_size = xm.mesh_reduce('total_size_reduce',total_size,reduce_fn)\n",
        "    mae_loss = xm.mesh_reduce('mae_loss_reduce',mae_loss,reduce_fn)\n",
        "    val_total_size = xm.mesh_reduce('val_total_size_reduce',val_total_size,reduce_fn)\n",
        "    test_mae_loss = xm.mesh_reduce('test_mae_loss_reduce',test_mae_loss,reduce_fn)\n",
        "    test_total_size = xm.mesh_reduce('test_total_size_reduce',test_total_size,reduce_fn)\n",
        "\n",
        "    if xm.is_master_ordinal():\n",
        "        print(test_total_size)\n",
        "        train_loss, val_mae, test_mae = training_loss/total_size, mae_loss/val_total_size, test_mae_loss/test_total_size\n",
        "        print(f'training loss is {train_loss}, val loss is {val_mae}, test loss is {test_mae}, time : {time.time() - start_time}, lr:{optimizer.param_groups[0][\"lr\"]}')\n",
        "\n",
        "\n",
        "    if xm.is_master_ordinal() and best_loss >= test_mae:\n",
        "        best_loss = test_mae\n",
        "        shutil.copy(f'{path}/{model_name}.bin', \\\n",
        "                    f'{path}/best_{model_name}.bin')"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMOUJmb1mnag",
        "outputId": "a5c63588-74f6-40ae-8d92-e2e57d7ffae8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "flags = {}\n",
        "flags['lr'] = 5e-4\n",
        "flags['batch_size'] = 32\n",
        "flags['num_workers'] = 4\n",
        "flags['num_epochs'] = 100\n",
        "flags['seed'] = 1234\n",
        "\n",
        "torch.set_default_tensor_type('torch.FloatTensor')\n",
        "xmp.spawn(map_fn, args=(flags,), nprocs=8, start_method='fork')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 49/49 [07:39<00:00,  9.38s/it, loss=0.24]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.3810]), val loss is tensor([11.8158]), test loss is tensor([8.6855]), time : 574.869416475296, lr:0.0005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 49/49 [05:31<00:00,  6.77s/it, loss=0.272]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.2652]), val loss is tensor([11.3471]), test loss is tensor([8.9927]), time : 396.16606163978577, lr:0.0005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 49/49 [05:51<00:00,  7.17s/it, loss=0.178]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.2514]), val loss is tensor([9.6850]), test loss is tensor([7.6258]), time : 413.3337061405182, lr:0.0005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 49/49 [05:34<00:00,  6.83s/it, loss=0.192]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.2333]), val loss is tensor([10.1135]), test loss is tensor([6.9377]), time : 395.90597319602966, lr:0.0005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|██████████| 49/49 [05:33<00:00,  6.80s/it, loss=0.182]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.2280]), val loss is tensor([8.5921]), test loss is tensor([7.3533]), time : 395.98440194129944, lr:0.0005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|██████████| 49/49 [05:51<00:00,  7.18s/it, loss=0.176]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.2145]), val loss is tensor([8.7300]), test loss is tensor([8.4887]), time : 414.64449191093445, lr:0.0005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|██████████| 49/49 [05:51<00:00,  7.18s/it, loss=0.204]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.2097]), val loss is tensor([10.9149]), test loss is tensor([8.4528]), time : 414.12093925476074, lr:0.0005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|██████████| 49/49 [05:50<00:00,  7.15s/it, loss=0.196]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1976]), val loss is tensor([10.0741]), test loss is tensor([8.4328]), time : 413.495178937912, lr:0.0005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|██████████| 49/49 [05:50<00:00,  7.15s/it, loss=0.173]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1980]), val loss is tensor([8.5235]), test loss is tensor([6.9309]), time : 412.0789692401886, lr:0.0005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|██████████| 49/49 [05:28<00:00,  6.70s/it, loss=0.202]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1947]), val loss is tensor([7.7863]), test loss is tensor([6.3375]), time : 391.86457562446594, lr:0.00025\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11: 100%|██████████| 49/49 [05:28<00:00,  6.71s/it, loss=0.137]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1830]), val loss is tensor([6.8958]), test loss is tensor([5.9850]), time : 389.72539710998535, lr:0.00025\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12: 100%|██████████| 49/49 [05:29<00:00,  6.72s/it, loss=0.165]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1790]), val loss is tensor([6.6812]), test loss is tensor([5.3675]), time : 392.3922452926636, lr:0.00025\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13: 100%|██████████| 49/49 [05:27<00:00,  6.69s/it, loss=0.14]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1771]), val loss is tensor([6.6072]), test loss is tensor([5.5278]), time : 391.38139724731445, lr:0.00025\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 14: 100%|██████████| 49/49 [05:51<00:00,  7.17s/it, loss=0.145]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1724]), val loss is tensor([6.9146]), test loss is tensor([5.0736]), time : 412.3677089214325, lr:0.00025\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 15: 100%|██████████| 49/49 [05:31<00:00,  6.77s/it, loss=0.16]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1695]), val loss is tensor([7.7756]), test loss is tensor([6.3596]), time : 395.2579069137573, lr:0.00025\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16: 100%|██████████| 49/49 [05:50<00:00,  7.16s/it, loss=0.146]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1712]), val loss is tensor([6.5010]), test loss is tensor([5.4409]), time : 412.7740547657013, lr:0.00025\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 17: 100%|██████████| 49/49 [05:51<00:00,  7.16s/it, loss=0.137]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1703]), val loss is tensor([7.3143]), test loss is tensor([5.8175]), time : 414.0702016353607, lr:0.00025\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 18: 100%|██████████| 49/49 [05:50<00:00,  7.16s/it, loss=0.122]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1715]), val loss is tensor([7.1286]), test loss is tensor([5.2055]), time : 413.5001573562622, lr:0.00025\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 19: 100%|██████████| 49/49 [05:50<00:00,  7.15s/it, loss=0.153]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1704]), val loss is tensor([7.7934]), test loss is tensor([6.2751]), time : 411.89405488967896, lr:0.00025\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 20: 100%|██████████| 49/49 [05:50<00:00,  7.14s/it, loss=0.146]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1668]), val loss is tensor([7.4013]), test loss is tensor([6.0406]), time : 411.29528975486755, lr:0.000125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 21: 100%|██████████| 49/49 [05:50<00:00,  7.15s/it, loss=0.161]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1607]), val loss is tensor([6.6059]), test loss is tensor([5.3381]), time : 412.6700186729431, lr:0.000125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 22: 100%|██████████| 49/49 [05:49<00:00,  7.14s/it, loss=0.139]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1579]), val loss is tensor([6.4429]), test loss is tensor([5.1243]), time : 411.4524579048157, lr:0.000125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 23: 100%|██████████| 49/49 [05:50<00:00,  7.16s/it, loss=0.142]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1578]), val loss is tensor([6.2830]), test loss is tensor([5.0525]), time : 411.9266119003296, lr:0.000125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 24: 100%|██████████| 49/49 [05:30<00:00,  6.74s/it, loss=0.152]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1548]), val loss is tensor([6.3810]), test loss is tensor([4.8161]), time : 391.5065360069275, lr:0.000125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 25: 100%|██████████| 49/49 [05:33<00:00,  6.80s/it, loss=0.137]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1548]), val loss is tensor([6.3706]), test loss is tensor([5.1727]), time : 395.37936544418335, lr:0.000125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 26: 100%|██████████| 49/49 [05:51<00:00,  7.17s/it, loss=0.147]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1548]), val loss is tensor([6.7044]), test loss is tensor([5.7703]), time : 412.70784068107605, lr:0.000125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 27: 100%|██████████| 49/49 [05:50<00:00,  7.16s/it, loss=0.147]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1568]), val loss is tensor([6.6048]), test loss is tensor([5.8013]), time : 412.88989448547363, lr:0.000125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 28: 100%|██████████| 49/49 [05:50<00:00,  7.16s/it, loss=0.134]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1551]), val loss is tensor([6.5664]), test loss is tensor([4.9896]), time : 413.0043046474457, lr:0.000125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 29: 100%|██████████| 49/49 [05:50<00:00,  7.16s/it, loss=0.15]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1556]), val loss is tensor([6.7231]), test loss is tensor([5.3832]), time : 414.1337251663208, lr:0.000125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 30: 100%|██████████| 49/49 [05:49<00:00,  7.14s/it, loss=0.129]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1537]), val loss is tensor([6.4191]), test loss is tensor([5.2719]), time : 411.42456340789795, lr:6.25e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 31: 100%|██████████| 49/49 [05:52<00:00,  7.18s/it, loss=0.139]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1493]), val loss is tensor([6.4311]), test loss is tensor([5.3994]), time : 414.37943053245544, lr:6.25e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 32: 100%|██████████| 49/49 [05:51<00:00,  7.17s/it, loss=0.144]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1493]), val loss is tensor([6.3138]), test loss is tensor([4.9356]), time : 411.50464940071106, lr:6.25e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 33: 100%|██████████| 49/49 [05:51<00:00,  7.17s/it, loss=0.156]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1478]), val loss is tensor([6.5852]), test loss is tensor([5.6247]), time : 414.0970108509064, lr:6.25e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 34: 100%|██████████| 49/49 [05:51<00:00,  7.16s/it, loss=0.135]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1468]), val loss is tensor([6.3722]), test loss is tensor([5.5146]), time : 413.1651465892792, lr:6.25e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 35: 100%|██████████| 49/49 [05:50<00:00,  7.16s/it, loss=0.129]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1463]), val loss is tensor([6.4777]), test loss is tensor([5.5149]), time : 413.3020405769348, lr:6.25e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 36: 100%|██████████| 49/49 [05:50<00:00,  7.15s/it, loss=0.148]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1467]), val loss is tensor([6.5749]), test loss is tensor([5.4755]), time : 411.7775378227234, lr:6.25e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 37: 100%|██████████| 49/49 [05:51<00:00,  7.17s/it, loss=0.124]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1459]), val loss is tensor([6.4199]), test loss is tensor([5.0798]), time : 414.0913276672363, lr:6.25e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 38: 100%|██████████| 49/49 [05:49<00:00,  7.14s/it, loss=0.132]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1455]), val loss is tensor([6.4688]), test loss is tensor([5.0834]), time : 412.3456723690033, lr:6.25e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 39: 100%|██████████| 49/49 [05:50<00:00,  7.15s/it, loss=0.133]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1454]), val loss is tensor([6.6532]), test loss is tensor([5.2072]), time : 411.9294912815094, lr:6.25e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 40: 100%|██████████| 49/49 [05:50<00:00,  7.16s/it, loss=0.125]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1436]), val loss is tensor([6.4485]), test loss is tensor([5.6480]), time : 412.655326128006, lr:3.125e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 41: 100%|██████████| 49/49 [05:50<00:00,  7.16s/it, loss=0.131]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1425]), val loss is tensor([6.1242]), test loss is tensor([5.1587]), time : 413.10407972335815, lr:3.125e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 42: 100%|██████████| 49/49 [05:51<00:00,  7.18s/it, loss=0.132]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1427]), val loss is tensor([6.1739]), test loss is tensor([4.7734]), time : 414.6008884906769, lr:3.125e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 43: 100%|██████████| 49/49 [05:28<00:00,  6.71s/it, loss=0.134]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1417]), val loss is tensor([6.1948]), test loss is tensor([4.7791]), time : 390.71587443351746, lr:3.125e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 44: 100%|██████████| 49/49 [05:52<00:00,  7.19s/it, loss=0.116]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1408]), val loss is tensor([6.2036]), test loss is tensor([5.2353]), time : 415.39941787719727, lr:3.125e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 45: 100%|██████████| 49/49 [05:50<00:00,  7.15s/it, loss=0.128]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1407]), val loss is tensor([6.1411]), test loss is tensor([4.8360]), time : 411.27524042129517, lr:3.125e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 46: 100%|██████████| 49/49 [05:52<00:00,  7.19s/it, loss=0.125]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1392]), val loss is tensor([6.1527]), test loss is tensor([5.1922]), time : 413.46413016319275, lr:3.125e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 47: 100%|██████████| 49/49 [05:51<00:00,  7.16s/it, loss=0.125]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1413]), val loss is tensor([6.2304]), test loss is tensor([4.8894]), time : 413.2374939918518, lr:3.125e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 48: 100%|██████████| 49/49 [05:50<00:00,  7.16s/it, loss=0.126]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1383]), val loss is tensor([6.2041]), test loss is tensor([5.0712]), time : 413.41062211990356, lr:3.125e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 49: 100%|██████████| 49/49 [05:51<00:00,  7.17s/it, loss=0.121]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1390]), val loss is tensor([6.1778]), test loss is tensor([5.1336]), time : 414.11673164367676, lr:3.125e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 50: 100%|██████████| 49/49 [05:50<00:00,  7.16s/it, loss=0.13]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1399]), val loss is tensor([6.1857]), test loss is tensor([4.7281]), time : 413.0182728767395, lr:1.5625e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 51: 100%|██████████| 49/49 [05:34<00:00,  6.82s/it, loss=0.129]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1387]), val loss is tensor([6.2066]), test loss is tensor([5.2751]), time : 395.9800639152527, lr:1.5625e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 52: 100%|██████████| 49/49 [05:51<00:00,  7.18s/it, loss=0.122]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1360]), val loss is tensor([6.1492]), test loss is tensor([4.9818]), time : 413.7427155971527, lr:1.5625e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 53: 100%|██████████| 49/49 [05:50<00:00,  7.16s/it, loss=0.117]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1363]), val loss is tensor([6.1386]), test loss is tensor([4.9905]), time : 411.79595971107483, lr:1.5625e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 54: 100%|██████████| 49/49 [05:51<00:00,  7.18s/it, loss=0.133]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1363]), val loss is tensor([6.1784]), test loss is tensor([5.0386]), time : 412.7449870109558, lr:1.5625e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 55: 100%|██████████| 49/49 [05:51<00:00,  7.18s/it, loss=0.11]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1353]), val loss is tensor([6.1764]), test loss is tensor([5.0760]), time : 413.04130244255066, lr:1.5625e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 56: 100%|██████████| 49/49 [05:51<00:00,  7.17s/it, loss=0.128]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1346]), val loss is tensor([6.1493]), test loss is tensor([5.0786]), time : 414.0259244441986, lr:1.5625e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 57: 100%|██████████| 49/49 [05:51<00:00,  7.18s/it, loss=0.12]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1360]), val loss is tensor([6.1742]), test loss is tensor([5.0342]), time : 414.5439555644989, lr:1.5625e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 58: 100%|██████████| 49/49 [05:50<00:00,  7.16s/it, loss=0.144]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1352]), val loss is tensor([6.1296]), test loss is tensor([5.0897]), time : 413.0235550403595, lr:1.5625e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 59: 100%|██████████| 49/49 [05:50<00:00,  7.15s/it, loss=0.112]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1358]), val loss is tensor([6.1591]), test loss is tensor([5.1358]), time : 412.0412414073944, lr:1.5625e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 60: 100%|██████████| 49/49 [05:50<00:00,  7.16s/it, loss=0.133]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1340]), val loss is tensor([6.1810]), test loss is tensor([5.0848]), time : 413.14010190963745, lr:7.8125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 61: 100%|██████████| 49/49 [05:50<00:00,  7.16s/it, loss=0.141]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1330]), val loss is tensor([6.1026]), test loss is tensor([5.0048]), time : 414.12085795402527, lr:7.8125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 62: 100%|██████████| 49/49 [05:51<00:00,  7.17s/it, loss=0.117]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1332]), val loss is tensor([6.1166]), test loss is tensor([4.8512]), time : 415.89820861816406, lr:7.8125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 63: 100%|██████████| 49/49 [05:51<00:00,  7.18s/it, loss=0.119]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1319]), val loss is tensor([6.1407]), test loss is tensor([4.9950]), time : 414.5193521976471, lr:7.8125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 64: 100%|██████████| 49/49 [05:49<00:00,  7.14s/it, loss=0.112]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1339]), val loss is tensor([6.0974]), test loss is tensor([4.8857]), time : 413.61982131004333, lr:7.8125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 65: 100%|██████████| 49/49 [05:50<00:00,  7.15s/it, loss=0.131]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1317]), val loss is tensor([6.1638]), test loss is tensor([4.9926]), time : 412.4505455493927, lr:7.8125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 66: 100%|██████████| 49/49 [05:50<00:00,  7.14s/it, loss=0.116]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1332]), val loss is tensor([6.1218]), test loss is tensor([4.9143]), time : 413.0911798477173, lr:7.8125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 67: 100%|██████████| 49/49 [05:51<00:00,  7.17s/it, loss=0.125]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1311]), val loss is tensor([6.1263]), test loss is tensor([4.9535]), time : 413.66946482658386, lr:7.8125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 68: 100%|██████████| 49/49 [05:51<00:00,  7.18s/it, loss=0.126]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1327]), val loss is tensor([6.1251]), test loss is tensor([4.9469]), time : 415.2355306148529, lr:7.8125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 69: 100%|██████████| 49/49 [05:51<00:00,  7.16s/it, loss=0.128]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1326]), val loss is tensor([6.1311]), test loss is tensor([4.8794]), time : 413.6729393005371, lr:7.8125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 70: 100%|██████████| 49/49 [05:51<00:00,  7.17s/it, loss=0.131]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1334]), val loss is tensor([6.1256]), test loss is tensor([4.9074]), time : 412.178817987442, lr:3.90625e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 71: 100%|██████████| 49/49 [05:49<00:00,  7.14s/it, loss=0.125]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1313]), val loss is tensor([6.1340]), test loss is tensor([4.8583]), time : 412.4348268508911, lr:3.90625e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 72: 100%|██████████| 49/49 [05:50<00:00,  7.15s/it, loss=0.121]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1301]), val loss is tensor([6.1195]), test loss is tensor([4.8732]), time : 412.61688780784607, lr:3.90625e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 73: 100%|██████████| 49/49 [05:51<00:00,  7.18s/it, loss=0.11]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1303]), val loss is tensor([6.1005]), test loss is tensor([4.8789]), time : 413.6741681098938, lr:3.90625e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 74: 100%|██████████| 49/49 [05:50<00:00,  7.15s/it, loss=0.135]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1313]), val loss is tensor([6.0952]), test loss is tensor([4.8946]), time : 413.2158966064453, lr:3.90625e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 75: 100%|██████████| 49/49 [05:51<00:00,  7.18s/it, loss=0.104]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1316]), val loss is tensor([6.0949]), test loss is tensor([4.9097]), time : 414.14880776405334, lr:3.90625e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 76: 100%|██████████| 49/49 [05:51<00:00,  7.16s/it, loss=0.123]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1304]), val loss is tensor([6.1203]), test loss is tensor([4.8943]), time : 412.19872212409973, lr:3.90625e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 77: 100%|██████████| 49/49 [05:51<00:00,  7.18s/it, loss=0.108]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1307]), val loss is tensor([6.1269]), test loss is tensor([4.9270]), time : 414.5564637184143, lr:3.90625e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 78: 100%|██████████| 49/49 [05:50<00:00,  7.16s/it, loss=0.127]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1315]), val loss is tensor([6.0894]), test loss is tensor([4.9267]), time : 412.7940273284912, lr:3.90625e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 79: 100%|██████████| 49/49 [05:51<00:00,  7.18s/it, loss=0.107]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1315]), val loss is tensor([6.0969]), test loss is tensor([4.8571]), time : 413.47597789764404, lr:3.90625e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 80: 100%|██████████| 49/49 [05:50<00:00,  7.16s/it, loss=0.125]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1306]), val loss is tensor([6.1151]), test loss is tensor([4.9826]), time : 413.43588757514954, lr:1.953125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 81: 100%|██████████| 49/49 [05:51<00:00,  7.18s/it, loss=0.101]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1310]), val loss is tensor([6.1049]), test loss is tensor([4.9341]), time : 413.70817708969116, lr:1.953125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 82: 100%|██████████| 49/49 [05:51<00:00,  7.17s/it, loss=0.112]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1295]), val loss is tensor([6.1170]), test loss is tensor([4.9061]), time : 414.71387100219727, lr:1.953125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 83: 100%|██████████| 49/49 [05:52<00:00,  7.18s/it, loss=0.123]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1320]), val loss is tensor([6.1042]), test loss is tensor([4.9424]), time : 414.55130338668823, lr:1.953125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 84: 100%|██████████| 49/49 [05:51<00:00,  7.18s/it, loss=0.117]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1300]), val loss is tensor([6.1148]), test loss is tensor([4.8526]), time : 415.0056676864624, lr:1.953125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 85: 100%|██████████| 49/49 [05:52<00:00,  7.19s/it, loss=0.114]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1300]), val loss is tensor([6.1186]), test loss is tensor([4.9483]), time : 415.25995349884033, lr:1.953125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 86: 100%|██████████| 49/49 [05:50<00:00,  7.15s/it, loss=0.102]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1301]), val loss is tensor([6.1160]), test loss is tensor([4.7911]), time : 412.98350954055786, lr:1.953125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 87: 100%|██████████| 49/49 [05:51<00:00,  7.18s/it, loss=0.119]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1297]), val loss is tensor([6.1186]), test loss is tensor([4.9063]), time : 412.97945261001587, lr:1.953125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 88: 100%|██████████| 49/49 [05:51<00:00,  7.17s/it, loss=0.123]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1292]), val loss is tensor([6.1257]), test loss is tensor([4.8506]), time : 414.91688442230225, lr:1.953125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 89: 100%|██████████| 49/49 [05:50<00:00,  7.15s/it, loss=0.125]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1298]), val loss is tensor([6.1237]), test loss is tensor([4.9054]), time : 412.96271681785583, lr:1.953125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 90: 100%|██████████| 49/49 [05:51<00:00,  7.18s/it, loss=0.119]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1300]), val loss is tensor([6.1181]), test loss is tensor([4.8689]), time : 412.9709084033966, lr:9.765625e-07\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 91: 100%|██████████| 49/49 [05:51<00:00,  7.18s/it, loss=0.102]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1296]), val loss is tensor([6.1234]), test loss is tensor([4.8790]), time : 415.1155400276184, lr:9.765625e-07\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 92: 100%|██████████| 49/49 [05:50<00:00,  7.15s/it, loss=0.114]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1300]), val loss is tensor([6.1216]), test loss is tensor([4.8908]), time : 412.7750995159149, lr:9.765625e-07\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 93: 100%|██████████| 49/49 [05:51<00:00,  7.18s/it, loss=0.114]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1290]), val loss is tensor([6.1210]), test loss is tensor([4.8992]), time : 413.30265283584595, lr:9.765625e-07\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 94: 100%|██████████| 49/49 [05:51<00:00,  7.17s/it, loss=0.114]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1283]), val loss is tensor([6.1347]), test loss is tensor([4.9403]), time : 414.0879695415497, lr:9.765625e-07\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 95: 100%|██████████| 49/49 [05:51<00:00,  7.18s/it, loss=0.117]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1294]), val loss is tensor([6.1325]), test loss is tensor([4.8991]), time : 412.7443754673004, lr:9.765625e-07\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 96: 100%|██████████| 49/49 [05:51<00:00,  7.18s/it, loss=0.119]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1293]), val loss is tensor([6.1173]), test loss is tensor([4.9208]), time : 415.10628485679626, lr:9.765625e-07\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 97: 100%|██████████| 49/49 [05:50<00:00,  7.15s/it, loss=0.116]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1306]), val loss is tensor([6.1214]), test loss is tensor([4.9037]), time : 413.4284739494324, lr:9.765625e-07\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 98: 100%|██████████| 49/49 [05:51<00:00,  7.16s/it, loss=0.112]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1299]), val loss is tensor([6.1243]), test loss is tensor([4.8813]), time : 413.2297465801239, lr:9.765625e-07\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 99: 100%|██████████| 49/49 [05:50<00:00,  7.16s/it, loss=0.116]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1292]), val loss is tensor([6.1184]), test loss is tensor([4.9070]), time : 411.4685935974121, lr:9.765625e-07\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 100: 100%|██████████| 49/49 [05:51<00:00,  7.18s/it, loss=0.124]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1283]), val loss is tensor([6.1160]), test loss is tensor([4.9668]), time : 414.658016204834, lr:4.8828125e-07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiC364ummwTk",
        "outputId": "f8a1d006-4a1c-47b0-a43d-b09048f8381d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%tb"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No traceback available to show.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWpoaeu5o_S3"
      },
      "source": [
        "#baseline 5.40 -> 4.42, resnet 50, 16, 3.82, resnet 50, 32, 4.22, se-resnet 4.51"
      ],
      "execution_count": 47,
      "outputs": []
    }
  ]
}